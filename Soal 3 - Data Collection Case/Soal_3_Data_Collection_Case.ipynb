{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hsyhHl3UX3V"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Web scraper for FortiGuard encyclopedia\"\"\"\n",
        "\n",
        "# Import dan mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalasi package eksternal (run di sel terpisah di Colab)\n",
        "pip install httpx BeautifulSoup4 polars tqdm\n",
        "pip install nest_asyncio\n",
        "\n",
        "# Import library yang dibutuhkan\n",
        "import asyncio\n",
        "import httpx\n",
        "from bs4 import BeautifulSoup\n",
        "import polars as pl\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from tqdm.asyncio import tqdm\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "zvZWReiZUdqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi agar asyncio bisa berjalan di Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ========================== KONFIGURASI DASAR ==========================\n",
        "BASE_URL = \"https://www.fortiguard.com\"\n",
        "LEVELS = [1, 2, 3, 4, 5]  # Level risiko IPS yang akan discan\n",
        "MAX_PAGES = [7, 34, 181, 451, 288]  # Jumlah halaman per level\n",
        "DATASETS_DIR = \"datasets\"\n",
        "os.makedirs(DATASETS_DIR, exist_ok=True)\n",
        "\n",
        "# Pengaturan scraping\n",
        "TIMEOUT = 20\n",
        "CONCURRENCY_LIMIT = 5\n",
        "DELAY_RANGE = (1, 2)\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "# Header untuk menyamarkan request sebagai browser biasa\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Upgrade-Insecure-Requests\": \"1\",\n",
        "    \"Cache-Control\": \"max-age=0\",\n",
        "}\n",
        "\n",
        "# ========================== TRACKING PROGRESS ==========================\n",
        "skipped_pages = {str(level): [] for level in LEVELS}\n",
        "completed_files = set()\n",
        "\n",
        "def check_existing_files():\n",
        "    \"\"\"Cek file yang sudah selesai sebelumnya\"\"\"\n",
        "    for level in LEVELS:\n",
        "        file_path = f\"{DATASETS_DIR}/fortilists{level}.csv\"\n",
        "        if os.path.exists(file_path):\n",
        "            completed_files.add(level)\n",
        "            print(f\"Level {level} already exists, continuing to next level.\")\n",
        "\n",
        "# ========================== FETCH HTML ==========================\n",
        "async def fetch(url, session=None):\n",
        "    \"\"\"Ambil HTML dari URL dengan error handling\"\"\"\n",
        "    close_session = False\n",
        "    if session is None:\n",
        "        session = httpx.AsyncClient(timeout=TIMEOUT, headers=HEADERS, follow_redirects=True)\n",
        "        close_session = True\n",
        "\n",
        "    async with semaphore:\n",
        "        try:\n",
        "            response = await session.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            content = response.text\n",
        "            if \"Access Denied\" in content or \"Robot or Virus Scan\" in content:\n",
        "                print(f\"Warning: Possible access denied for {url}.\")\n",
        "                return None\n",
        "\n",
        "            await asyncio.sleep(random.uniform(*DELAY_RANGE))\n",
        "            return content\n",
        "\n",
        "        except (httpx.HTTPError, httpx.RequestError, httpx.ReadTimeout) as e:\n",
        "            print(f\"Error fetching {url}: {str(e)}.\")\n",
        "            return None\n",
        "\n",
        "        finally:\n",
        "            if close_session:\n",
        "                await session.aclose()\n",
        "\n",
        "# ========================== SCRAPER FUNGSI SPESIFIK ==========================\n",
        "async def extract_detail_url(row):\n",
        "    \"\"\"Ambil link detail dari baris onclick\"\"\"\n",
        "    try:\n",
        "        onclick = row.attrs.get(\"onclick\")\n",
        "        if onclick:\n",
        "            partial_link = onclick.split(\"'\")[1]\n",
        "            return BASE_URL + partial_link\n",
        "    except (IndexError, ValueError) as e:\n",
        "        print(f\"Error parsing onclick: {e}\")\n",
        "    return None\n",
        "\n",
        "async def scrape_detail(url, session):\n",
        "    \"\"\"Scrape halaman detail dan ambil judul\"\"\"\n",
        "    html = await fetch(url, session)\n",
        "    if html is None:\n",
        "        return {\"title\": \"N/A\", \"link\": url}\n",
        "\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    title = \"N/A\"\n",
        "\n",
        "    # Metode pencarian judul (berlapis)\n",
        "    title_tag = soup.find(\"h1\", class_=\"title\")\n",
        "    if title_tag:\n",
        "        text_node = title_tag.get_text(strip=True)\n",
        "        if text_node:\n",
        "            title = text_node\n",
        "\n",
        "    if title == \"N/A\":\n",
        "        info_box = soup.select_one(\".info-box h2\")\n",
        "        if info_box:\n",
        "            title = info_box.get_text(strip=True)\n",
        "\n",
        "    if title == \"N/A\":\n",
        "        id_title = soup.select_one(\"#main-title\")\n",
        "        if id_title:\n",
        "            title = id_title.get_text(strip=True)\n",
        "\n",
        "    if title == \"N/A\":\n",
        "        meta_title = soup.find(\"meta\", property=\"og:title\")\n",
        "        if meta_title:\n",
        "            title = meta_title.get(\"content\", \"N/A\")\n",
        "\n",
        "    return {\"title\": title, \"link\": url}\n",
        "\n",
        "# ========================== SCRAPE PER PAGE ==========================\n",
        "async def scrape_page(level, page, session):\n",
        "    \"\"\"Scrape satu halaman listing berdasarkan level dan page\"\"\"\n",
        "    url = f\"{BASE_URL}/encyclopedia?type=ips&risk={level}&page={page}\"\n",
        "    html = await fetch(url, session)\n",
        "\n",
        "    if html is None:\n",
        "        skipped_pages[str(level)].append(page)\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    rows = soup.select(\"section.table-body div.row[onclick]\")\n",
        "\n",
        "    # Jika selector utama tidak ada, coba alternatif\n",
        "    if not rows:\n",
        "        print(f\"Warning: No rows found on level {level}, page {page}. Checking alternative selectors...\")\n",
        "        rows = soup.select(\"div.row[onclick], tr[onclick], .item[onclick]\")\n",
        "        if not rows:\n",
        "            print(f\"Still no rows found for level {level}, page {page}. Marking as skipped.\")\n",
        "            skipped_pages[str(level)].append(page)\n",
        "            return []\n",
        "\n",
        "    print(f\"Found {len(rows)} items on level {level}, page {page}\")\n",
        "\n",
        "    detail_urls = []\n",
        "    for row in rows:\n",
        "        detail_url = await extract_detail_url(row)\n",
        "        if detail_url:\n",
        "            detail_urls.append(detail_url)\n",
        "\n",
        "    if not detail_urls:\n",
        "        skipped_pages[str(level)].append(page)\n",
        "        return []\n",
        "\n",
        "    # Jalankan scraping detail secara paralel\n",
        "    tasks = [scrape_detail(url, session) for url in detail_urls]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return results\n",
        "\n",
        "# ========================== SCRAPE PER LEVEL ==========================\n",
        "async def scrape_level_in_batches(level, max_page):\n",
        "    \"\"\"Scrape seluruh halaman pada satu level secara batch\"\"\"\n",
        "    if level in completed_files:\n",
        "        print(f\"Skipping level {level} as it's already completed.\")\n",
        "        return []\n",
        "\n",
        "    all_data = []\n",
        "    async with httpx.AsyncClient(timeout=TIMEOUT, headers=HEADERS, follow_redirects=True) as session:\n",
        "        for start_page in range(1, max_page + 1, BATCH_SIZE):\n",
        "            end_page = min(start_page + BATCH_SIZE - 1, max_page)\n",
        "            batch_tasks = [scrape_page(level, page, session) for page in range(start_page, end_page + 1)]\n",
        "\n",
        "            with tqdm(total=len(batch_tasks), desc=f\"Level {level} Batch {start_page}-{end_page}\", ncols=100) as pbar:\n",
        "                for task in asyncio.as_completed(batch_tasks):\n",
        "                    page_data = await task\n",
        "                    all_data.extend(page_data)\n",
        "                    pbar.update(1)\n",
        "\n",
        "                    # Simpan hasil sementara\n",
        "                    if len(all_data) % 100 == 0 and all_data:\n",
        "                        partial_df = pl.DataFrame(all_data)\n",
        "                        partial_df.write_csv(f\"{DATASETS_DIR}/fortilists{level}_partial.csv\")\n",
        "\n",
        "            # Simpan halaman yang dilewati\n",
        "            with open(f\"{DATASETS_DIR}/skipped.json\", \"w\") as f:\n",
        "                json.dump(skipped_pages, f, indent=2)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# ========================== MAIN FUNCTION ==========================\n",
        "async def main():\n",
        "    \"\"\"Fungsi utama yang menjalankan seluruh proses\"\"\"\n",
        "    global semaphore\n",
        "    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "    start_time = time.time()\n",
        "\n",
        "    check_existing_files()\n",
        "\n",
        "    skipped_file = f\"{DATASETS_DIR}/skipped.json\"\n",
        "    if os.path.exists(skipped_file):\n",
        "        with open(skipped_file, \"r\") as f:\n",
        "            try:\n",
        "                skipped_data = json.load(f)\n",
        "                skipped_pages.update(skipped_data)\n",
        "                print(f\"Loaded skipped pages: {skipped_pages}\")\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Error reading skipped.json, starting fresh.\")\n",
        "\n",
        "    for level, max_page in zip(LEVELS, MAX_PAGES):\n",
        "        if level not in completed_files:\n",
        "            data = await scrape_level_in_batches(level, max_page)\n",
        "\n",
        "            if data:\n",
        "                df = pl.DataFrame(data)\n",
        "                df = df.unique(subset=[\"link\"], maintain_order=True)\n",
        "                output_file = f\"{DATASETS_DIR}/fortilists{level}.csv\"\n",
        "                df.write_csv(output_file)\n",
        "                print(f\"Saved {len(df)} records to {output_file}\")\n",
        "\n",
        "                partial_file = f\"{DATASETS_DIR}/fortilists{level}_partial.csv\"\n",
        "                if os.path.exists(partial_file):\n",
        "                    os.remove(partial_file)\n",
        "\n",
        "    # Simpan hasil halaman yang dilewati\n",
        "    with open(f\"{DATASETS_DIR}/skipped.json\", \"w\") as f:\n",
        "        json.dump(skipped_pages, f, indent=2)\n",
        "\n",
        "    # Statistik akhir\n",
        "    print(\"\\nScraping Statistics:\")\n",
        "    total_skipped = sum(len(pages) for pages in skipped_pages.values())\n",
        "    print(f\"Total skipped pages: {total_skipped}\")\n",
        "\n",
        "    for level in LEVELS:\n",
        "        file_path = f\"{DATASETS_DIR}/fortilists{level}.csv\"\n",
        "        if os.path.exists(file_path):\n",
        "            df = pl.read_csv(file_path)\n",
        "            na_count = df.filter(pl.col(\"title\") == \"N/A\").height\n",
        "            total_count = df.height\n",
        "            na_percentage = (na_count / total_count * 100) if total_count > 0 else 0\n",
        "            print(f\"Level {level}: {total_count} records, {na_count} N/A titles ({na_percentage:.2f}%)\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    hours, remainder = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    print(f\"Scraping completed in {int(hours)}h {int(minutes)}m {seconds:.2f}s.\")\n",
        "\n",
        "# Eksekusi fungsi utama\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "xxYz5KLmUgUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLjJ3BrLUgW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58daljipUgZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPZaHcOTUgbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1drK9QHtUgd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vyrbIMftUggN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}